{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nature Conservance Fish Identification Using CNN Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980M (CNMeM is enabled with initial size: 90.0% of memory, cuDNN 5105)\n",
      "/home/robert/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from glob import iglob\n",
    "from models import Vgg16BN, Inception, Resnet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_HOME_DIR = ROOT_DIR + '/data'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "data_path = DATA_HOME_DIR + '/' \n",
    "split_train_path = data_path + '/train/'\n",
    "full_train_path = data_path + '/train_full/'\n",
    "valid_path = data_path + '/valid/'\n",
    "test_path = DATA_HOME_DIR + '/test/'\n",
    "saved_model_path = ROOT_DIR + '/models/'\n",
    "submission_path = ROOT_DIR + '/submissions/'\n",
    "\n",
    "# data\n",
    "batch_size = 16\n",
    "nb_split_train_samples = 3277\n",
    "nb_full_train_samples = 3785\n",
    "nb_valid_samples = 500\n",
    "nb_test_samples = 1000\n",
    "classes = [\"ALB\", \"BET\", \"DOL\", \"LAG\", \"NoF\", \"OTHER\", \"SHARK\", \"YFT\"]\n",
    "nb_classes = len(classes)\n",
    "\n",
    "# model\n",
    "nb_runs = 5\n",
    "nb_epoch = 30\n",
    "nb_aug = 5\n",
    "dropout = 0.4\n",
    "clip = 0.01\n",
    "use_val = True\n",
    "archs = [\"inception\"]\n",
    "\n",
    "models = {\n",
    "    \"vggbn\": Vgg16BN(size=(270, 480), n_classes=nb_classes, lr=0.001,\n",
    "                           batch_size=batch_size, dropout=dropout),\n",
    "    \"inception\": Inception(size=(299, 299), n_classes=nb_classes,\n",
    "                           lr=0.001, batch_size=batch_size),\n",
    "    \"resnet\": Resnet50(size=(270, 480), n_classes=nb_classes, lr=0.001,\n",
    "                    batch_size=batch_size, dropout=dropout)\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a training loop that runs `nb_runs` times and trains a model for each architecture we've specified. \n",
    "\n",
    "We have the option to use validation data or the full training set -- if we use \n",
    "the former, the best model from each training loop will be saved and the path appended to our `models` list for \n",
    "later use; if the latter, we just save the weights from the last epoch of each training loop, since there's no validation monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(parent_model, model_str):\n",
    "    parent_model.build()    \n",
    "    model_fn = saved_model_path + '{val_loss:.2f}-loss_{epoch}epoch_' + model_str\n",
    "    ckpt = ModelCheckpoint(filepath=model_fn, monitor='val_loss',\n",
    "                           save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    if use_val:\n",
    "        parent_model.fit_val(split_train_path, valid_path, nb_trn_samples=nb_split_train_samples, \n",
    "                             nb_val_samples=nb_valid_samples, nb_epoch=nb_epoch, callbacks=[ckpt], aug=nb_aug)\n",
    "\n",
    "        model_path = max(iglob(saved_model_path + '*.h5'), key=os.path.getctime)\n",
    "        return model_path\n",
    "    \n",
    "    model_fn = saved_model_path + '{}epoch_'.format(nb_epoch) + model_str\n",
    "    parent_model.fit_full(full_train_path, nb_trn_samples=nb_full_train_samples, nb_epoch=nb_epoch, aug=nb_aug)\n",
    "    model.save_weights(model_fn)\n",
    "    del parent_model.model \n",
    "    \n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Run 1 of 5...\n",
      "\n",
      "Training inception model...\n",
      "\n",
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Epoch 1/30\n",
      "185s - loss: 2.0277 - acc: 0.2362 - val_loss: 1.9919 - val_acc: 0.2600\n",
      "Epoch 2/30\n",
      "102s - loss: 1.7884 - acc: 0.3760 - val_loss: 1.8440 - val_acc: 0.3500\n",
      "Epoch 3/30\n",
      "96s - loss: 1.6888 - acc: 0.4208 - val_loss: 1.6998 - val_acc: 0.4040\n",
      "Epoch 4/30\n",
      "95s - loss: 1.5989 - acc: 0.4519 - val_loss: 1.8111 - val_acc: 0.3760\n",
      "Epoch 5/30\n",
      "95s - loss: 1.5422 - acc: 0.4721 - val_loss: 1.7354 - val_acc: 0.4220\n",
      "Epoch 6/30\n",
      "98s - loss: 1.4904 - acc: 0.5038 - val_loss: 1.6087 - val_acc: 0.4620\n",
      "Epoch 7/30\n",
      "95s - loss: 1.4397 - acc: 0.5258 - val_loss: 1.4943 - val_acc: 0.5020\n",
      "Epoch 8/30\n",
      "93s - loss: 1.4218 - acc: 0.5197 - val_loss: 1.4678 - val_acc: 0.5140\n",
      "Epoch 9/30\n",
      "92s - loss: 1.3908 - acc: 0.5346 - val_loss: 1.5182 - val_acc: 0.4880\n",
      "Epoch 10/30\n",
      "93s - loss: 1.3754 - acc: 0.5423 - val_loss: 1.5444 - val_acc: 0.4820\n",
      "Epoch 11/30\n",
      "93s - loss: 1.3300 - acc: 0.5590 - val_loss: 1.5585 - val_acc: 0.4700\n",
      "Epoch 12/30\n",
      "94s - loss: 1.3151 - acc: 0.5612 - val_loss: 1.5172 - val_acc: 0.4920\n",
      "Epoch 13/30\n",
      "94s - loss: 1.2857 - acc: 0.5728 - val_loss: 1.4991 - val_acc: 0.4980\n",
      "Epoch 14/30\n",
      "93s - loss: 1.2643 - acc: 0.5801 - val_loss: 1.3918 - val_acc: 0.5320\n",
      "Epoch 15/30\n",
      "93s - loss: 1.2496 - acc: 0.5844 - val_loss: 1.3464 - val_acc: 0.5440\n",
      "Epoch 16/30\n",
      "94s - loss: 1.2230 - acc: 0.5972 - val_loss: 1.2924 - val_acc: 0.5600\n",
      "Epoch 17/30\n",
      "93s - loss: 1.2112 - acc: 0.5978 - val_loss: 1.3317 - val_acc: 0.5600\n",
      "Epoch 18/30\n",
      "95s - loss: 1.1956 - acc: 0.6060 - val_loss: 1.2410 - val_acc: 0.5640\n",
      "Epoch 19/30\n",
      "94s - loss: 1.1893 - acc: 0.6015 - val_loss: 1.1963 - val_acc: 0.6060\n",
      "Epoch 20/30\n",
      "91s - loss: 1.1762 - acc: 0.6088 - val_loss: 1.1672 - val_acc: 0.6340\n",
      "Epoch 21/30\n",
      "93s - loss: 1.1429 - acc: 0.6259 - val_loss: 1.1717 - val_acc: 0.6360\n",
      "Epoch 22/30\n",
      "94s - loss: 1.1435 - acc: 0.6207 - val_loss: 1.2984 - val_acc: 0.5660\n",
      "Epoch 23/30\n",
      "100s - loss: 1.1296 - acc: 0.6320 - val_loss: 1.1998 - val_acc: 0.6300\n",
      "Epoch 24/30\n",
      "95s - loss: 1.1270 - acc: 0.6277 - val_loss: 1.1992 - val_acc: 0.6320\n",
      "Epoch 25/30\n",
      "96s - loss: 1.1124 - acc: 0.6372 - val_loss: 1.2072 - val_acc: 0.6080\n",
      "Epoch 26/30\n",
      "97s - loss: 1.1164 - acc: 0.6265 - val_loss: 1.2691 - val_acc: 0.6060\n",
      "Epoch 27/30\n",
      "99s - loss: 1.0925 - acc: 0.6445 - val_loss: 1.1870 - val_acc: 0.6400\n",
      "Epoch 28/30\n",
      "101s - loss: 1.0827 - acc: 0.6414 - val_loss: 1.2194 - val_acc: 0.6080\n",
      "Epoch 29/30\n",
      "93s - loss: 1.0670 - acc: 0.6488 - val_loss: 1.1759 - val_acc: 0.6320\n",
      "Epoch 30/30\n",
      "95s - loss: 1.0637 - acc: 0.6533 - val_loss: 1.1273 - val_acc: 0.6660\n",
      "Starting Training Run 2 of 5...\n",
      "\n",
      "Training inception model...\n",
      "\n",
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Epoch 1/30\n",
      "95s - loss: 1.9969 - acc: 0.2377 - val_loss: 2.1849 - val_acc: 0.1880\n",
      "Epoch 2/30\n",
      "94s - loss: 1.7812 - acc: 0.3671 - val_loss: 1.9328 - val_acc: 0.2980\n",
      "Epoch 3/30\n",
      "93s - loss: 1.6806 - acc: 0.4095 - val_loss: 1.8616 - val_acc: 0.3460\n",
      "Epoch 4/30\n",
      "92s - loss: 1.5858 - acc: 0.4596 - val_loss: 1.9698 - val_acc: 0.3320\n",
      "Epoch 5/30\n",
      "96s - loss: 1.5418 - acc: 0.4696 - val_loss: 1.9801 - val_acc: 0.3280\n",
      "Epoch 6/30\n",
      "95s - loss: 1.4919 - acc: 0.4910 - val_loss: 1.7540 - val_acc: 0.4200\n",
      "Epoch 7/30\n",
      "96s - loss: 1.4528 - acc: 0.5011 - val_loss: 1.6690 - val_acc: 0.4340\n",
      "Epoch 8/30\n",
      "96s - loss: 1.4033 - acc: 0.5255 - val_loss: 1.6064 - val_acc: 0.4680\n",
      "Epoch 9/30\n",
      "96s - loss: 1.3697 - acc: 0.5368 - val_loss: 1.6576 - val_acc: 0.4340\n",
      "Epoch 10/30\n",
      "96s - loss: 1.3452 - acc: 0.5450 - val_loss: 1.6916 - val_acc: 0.4140\n",
      "Epoch 11/30\n",
      "97s - loss: 1.3297 - acc: 0.5520 - val_loss: 1.5244 - val_acc: 0.4880\n",
      "Epoch 12/30\n",
      "97s - loss: 1.3375 - acc: 0.5386 - val_loss: 1.5228 - val_acc: 0.4860\n",
      "Epoch 13/30\n",
      "93s - loss: 1.2942 - acc: 0.5581 - val_loss: 1.5430 - val_acc: 0.4700\n",
      "Epoch 14/30\n",
      "90s - loss: 1.2727 - acc: 0.5648 - val_loss: 1.5033 - val_acc: 0.4960\n",
      "Epoch 15/30\n",
      "90s - loss: 1.2742 - acc: 0.5719 - val_loss: 1.5292 - val_acc: 0.4880\n",
      "Epoch 16/30\n",
      "97s - loss: 1.2349 - acc: 0.5783 - val_loss: 1.5739 - val_acc: 0.4420\n",
      "Epoch 17/30\n",
      "94s - loss: 1.2144 - acc: 0.5978 - val_loss: 1.3657 - val_acc: 0.5180\n",
      "Epoch 18/30\n",
      "93s - loss: 1.1881 - acc: 0.5990 - val_loss: 1.3959 - val_acc: 0.5100\n",
      "Epoch 19/30\n",
      "99s - loss: 1.1858 - acc: 0.5923 - val_loss: 1.2650 - val_acc: 0.5720\n",
      "Epoch 20/30\n",
      "101s - loss: 1.1703 - acc: 0.6134 - val_loss: 1.2856 - val_acc: 0.5580\n",
      "Epoch 21/30\n",
      "99s - loss: 1.1640 - acc: 0.6109 - val_loss: 1.3303 - val_acc: 0.5380\n",
      "Epoch 22/30\n",
      "101s - loss: 1.1307 - acc: 0.6213 - val_loss: 1.3263 - val_acc: 0.5540\n",
      "Epoch 23/30\n",
      "125s - loss: 1.1139 - acc: 0.6295 - val_loss: 1.2500 - val_acc: 0.5880\n",
      "Epoch 24/30\n",
      "102s - loss: 1.1304 - acc: 0.6176 - val_loss: 1.3462 - val_acc: 0.5640\n",
      "Epoch 25/30\n",
      "100s - loss: 1.1009 - acc: 0.6372 - val_loss: 1.2277 - val_acc: 0.5780\n",
      "Epoch 26/30\n",
      "99s - loss: 1.1106 - acc: 0.6213 - val_loss: 1.2483 - val_acc: 0.5840\n",
      "Epoch 27/30\n",
      "96s - loss: 1.0967 - acc: 0.6301 - val_loss: 1.2371 - val_acc: 0.5680\n",
      "Epoch 28/30\n",
      "97s - loss: 1.0901 - acc: 0.6390 - val_loss: 1.1810 - val_acc: 0.6080\n",
      "Epoch 29/30\n",
      "97s - loss: 1.0812 - acc: 0.6399 - val_loss: 1.1790 - val_acc: 0.6000\n",
      "Epoch 30/30\n",
      "97s - loss: 1.0755 - acc: 0.6424 - val_loss: 1.1696 - val_acc: 0.6120\n",
      "Starting Training Run 3 of 5...\n",
      "\n",
      "Training inception model...\n",
      "\n",
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Epoch 1/30\n",
      "100s - loss: 2.1253 - acc: 0.1730 - val_loss: 2.0160 - val_acc: 0.2440\n",
      "Epoch 2/30\n",
      "96s - loss: 1.8531 - acc: 0.3384 - val_loss: 1.8822 - val_acc: 0.3300\n",
      "Epoch 3/30\n",
      "97s - loss: 1.7107 - acc: 0.4107 - val_loss: 1.9740 - val_acc: 0.3140\n",
      "Epoch 4/30\n",
      "97s - loss: 1.6255 - acc: 0.4504 - val_loss: 1.8688 - val_acc: 0.3540\n",
      "Epoch 5/30\n",
      "98s - loss: 1.5640 - acc: 0.4702 - val_loss: 1.6974 - val_acc: 0.4340\n",
      "Epoch 6/30\n",
      "97s - loss: 1.5290 - acc: 0.4825 - val_loss: 1.7119 - val_acc: 0.4020\n",
      "Epoch 7/30\n",
      "97s - loss: 1.4649 - acc: 0.5005 - val_loss: 1.6770 - val_acc: 0.4600\n",
      "Epoch 8/30\n",
      "98s - loss: 1.4356 - acc: 0.5240 - val_loss: 1.5217 - val_acc: 0.5120\n",
      "Epoch 9/30\n",
      "97s - loss: 1.4176 - acc: 0.5172 - val_loss: 1.5569 - val_acc: 0.5080\n",
      "Epoch 10/30\n",
      "99s - loss: 1.3894 - acc: 0.5426 - val_loss: 1.5358 - val_acc: 0.5060\n",
      "Epoch 11/30\n",
      "96s - loss: 1.3476 - acc: 0.5484 - val_loss: 1.4062 - val_acc: 0.5520\n",
      "Epoch 12/30\n",
      "98s - loss: 1.3166 - acc: 0.5719 - val_loss: 1.5509 - val_acc: 0.4900\n",
      "Epoch 13/30\n",
      "97s - loss: 1.3049 - acc: 0.5572 - val_loss: 1.3330 - val_acc: 0.5740\n",
      "Epoch 14/30\n",
      "97s - loss: 1.2656 - acc: 0.5777 - val_loss: 1.3666 - val_acc: 0.5700\n",
      "Epoch 15/30\n",
      "97s - loss: 1.2622 - acc: 0.5691 - val_loss: 1.4181 - val_acc: 0.5380\n",
      "Epoch 16/30\n",
      "97s - loss: 1.2428 - acc: 0.5853 - val_loss: 1.3514 - val_acc: 0.5520\n",
      "Epoch 17/30\n",
      "97s - loss: 1.2224 - acc: 0.5938 - val_loss: 1.3620 - val_acc: 0.5620\n",
      "Epoch 18/30\n",
      "99s - loss: 1.1878 - acc: 0.6082 - val_loss: 1.3867 - val_acc: 0.5580\n",
      "Epoch 19/30\n",
      "97s - loss: 1.1892 - acc: 0.5990 - val_loss: 1.2495 - val_acc: 0.6260\n",
      "Epoch 20/30\n",
      "97s - loss: 1.1791 - acc: 0.6045 - val_loss: 1.3032 - val_acc: 0.5580\n",
      "Epoch 21/30\n",
      "96s - loss: 1.1835 - acc: 0.6100 - val_loss: 1.3893 - val_acc: 0.5380\n",
      "Epoch 22/30\n",
      "97s - loss: 1.1465 - acc: 0.6219 - val_loss: 1.3431 - val_acc: 0.5720\n",
      "Epoch 23/30\n",
      "97s - loss: 1.1295 - acc: 0.6265 - val_loss: 1.2766 - val_acc: 0.5880\n",
      "Epoch 24/30\n",
      "98s - loss: 1.1265 - acc: 0.6237 - val_loss: 1.1799 - val_acc: 0.6300\n",
      "Epoch 25/30\n",
      "96s - loss: 1.1094 - acc: 0.6369 - val_loss: 1.2286 - val_acc: 0.5920\n",
      "Epoch 26/30\n",
      "97s - loss: 1.1109 - acc: 0.6320 - val_loss: 1.2375 - val_acc: 0.5880\n",
      "Epoch 27/30\n",
      "97s - loss: 1.0777 - acc: 0.6442 - val_loss: 1.1837 - val_acc: 0.6380\n",
      "Epoch 28/30\n",
      "98s - loss: 1.0941 - acc: 0.6308 - val_loss: 1.1515 - val_acc: 0.6300\n",
      "Epoch 29/30\n",
      "97s - loss: 1.0684 - acc: 0.6491 - val_loss: 1.1000 - val_acc: 0.6560\n",
      "Epoch 30/30\n",
      "97s - loss: 1.0429 - acc: 0.6546 - val_loss: 1.1155 - val_acc: 0.6480\n",
      "Starting Training Run 4 of 5...\n",
      "\n",
      "Training inception model...\n",
      "\n",
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Epoch 1/30\n",
      "102s - loss: 2.0414 - acc: 0.2197 - val_loss: 2.1281 - val_acc: 0.2200\n",
      "Epoch 2/30\n",
      "97s - loss: 1.8315 - acc: 0.3409 - val_loss: 1.9381 - val_acc: 0.3240\n",
      "Epoch 3/30\n",
      "97s - loss: 1.6957 - acc: 0.4175 - val_loss: 2.0116 - val_acc: 0.3100\n",
      "Epoch 4/30\n",
      "96s - loss: 1.6262 - acc: 0.4416 - val_loss: 1.9277 - val_acc: 0.3480\n",
      "Epoch 5/30\n",
      "97s - loss: 1.5654 - acc: 0.4617 - val_loss: 1.7947 - val_acc: 0.4140\n",
      "Epoch 6/30\n",
      "96s - loss: 1.5294 - acc: 0.4883 - val_loss: 1.6984 - val_acc: 0.4380\n",
      "Epoch 7/30\n",
      "97s - loss: 1.4650 - acc: 0.5169 - val_loss: 1.6779 - val_acc: 0.4400\n",
      "Epoch 8/30\n",
      "97s - loss: 1.4431 - acc: 0.5151 - val_loss: 1.5777 - val_acc: 0.4920\n",
      "Epoch 9/30\n",
      "98s - loss: 1.4071 - acc: 0.5316 - val_loss: 1.6420 - val_acc: 0.4460\n",
      "Epoch 10/30\n",
      "97s - loss: 1.3798 - acc: 0.5313 - val_loss: 1.6379 - val_acc: 0.4380\n",
      "Epoch 11/30\n",
      "97s - loss: 1.3434 - acc: 0.5453 - val_loss: 1.4414 - val_acc: 0.5460\n",
      "Epoch 12/30\n",
      "97s - loss: 1.3274 - acc: 0.5453 - val_loss: 1.4806 - val_acc: 0.5080\n",
      "Epoch 13/30\n",
      "97s - loss: 1.3111 - acc: 0.5633 - val_loss: 1.4576 - val_acc: 0.5280\n",
      "Epoch 14/30\n",
      "97s - loss: 1.2741 - acc: 0.5658 - val_loss: 1.4347 - val_acc: 0.5220\n",
      "Epoch 15/30\n",
      "97s - loss: 1.2605 - acc: 0.5755 - val_loss: 1.4269 - val_acc: 0.5320\n",
      "Epoch 16/30\n",
      "97s - loss: 1.2486 - acc: 0.5862 - val_loss: 1.4593 - val_acc: 0.5280\n",
      "Epoch 17/30\n",
      "97s - loss: 1.2186 - acc: 0.5810 - val_loss: 1.4119 - val_acc: 0.5560\n",
      "Epoch 18/30\n",
      "97s - loss: 1.2159 - acc: 0.5978 - val_loss: 1.2831 - val_acc: 0.6080\n",
      "Epoch 19/30\n",
      "96s - loss: 1.1953 - acc: 0.5999 - val_loss: 1.3706 - val_acc: 0.5700\n",
      "Epoch 20/30\n",
      "96s - loss: 1.1786 - acc: 0.6033 - val_loss: 1.3519 - val_acc: 0.5880\n",
      "Epoch 21/30\n",
      "97s - loss: 1.1660 - acc: 0.6109 - val_loss: 1.2711 - val_acc: 0.5960\n",
      "Epoch 22/30\n",
      "99s - loss: 1.1636 - acc: 0.6161 - val_loss: 1.2393 - val_acc: 0.6240\n",
      "Epoch 23/30\n",
      "97s - loss: 1.1486 - acc: 0.6182 - val_loss: 1.2221 - val_acc: 0.6220\n",
      "Epoch 24/30\n",
      "102s - loss: 1.1286 - acc: 0.6286 - val_loss: 1.2974 - val_acc: 0.6140\n",
      "Epoch 25/30\n",
      "101s - loss: 1.1053 - acc: 0.6323 - val_loss: 1.3248 - val_acc: 0.5920\n",
      "Epoch 26/30\n",
      "103s - loss: 1.1127 - acc: 0.6338 - val_loss: 1.1899 - val_acc: 0.6520\n",
      "Epoch 27/30\n",
      "97s - loss: 1.0948 - acc: 0.6308 - val_loss: 1.2286 - val_acc: 0.6260\n",
      "Epoch 28/30\n",
      "100s - loss: 1.0954 - acc: 0.6323 - val_loss: 1.1762 - val_acc: 0.6400\n",
      "Epoch 29/30\n",
      "96s - loss: 1.0613 - acc: 0.6512 - val_loss: 1.1499 - val_acc: 0.6480\n",
      "Epoch 30/30\n",
      "96s - loss: 1.0724 - acc: 0.6427 - val_loss: 1.1492 - val_acc: 0.6360\n",
      "Starting Training Run 5 of 5...\n",
      "\n",
      "Training inception model...\n",
      "\n",
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Epoch 1/30\n",
      "101s - loss: 2.0628 - acc: 0.2118 - val_loss: 2.1221 - val_acc: 0.2420\n",
      "Epoch 2/30\n",
      "96s - loss: 1.7930 - acc: 0.3653 - val_loss: 2.1532 - val_acc: 0.2640\n",
      "Epoch 3/30\n",
      "96s - loss: 1.6852 - acc: 0.4202 - val_loss: 1.8421 - val_acc: 0.4260\n",
      "Epoch 4/30\n",
      "96s - loss: 1.6240 - acc: 0.4480 - val_loss: 1.8895 - val_acc: 0.3860\n",
      "Epoch 5/30\n",
      "96s - loss: 1.5755 - acc: 0.4593 - val_loss: 1.8738 - val_acc: 0.3940\n",
      "Epoch 6/30\n",
      "97s - loss: 1.4946 - acc: 0.4968 - val_loss: 1.7269 - val_acc: 0.4300\n",
      "Epoch 7/30\n",
      "96s - loss: 1.4658 - acc: 0.4965 - val_loss: 1.7582 - val_acc: 0.4220\n",
      "Epoch 8/30\n",
      "96s - loss: 1.4228 - acc: 0.5188 - val_loss: 1.5689 - val_acc: 0.4880\n",
      "Epoch 9/30\n",
      "97s - loss: 1.3985 - acc: 0.5230 - val_loss: 1.5862 - val_acc: 0.5140\n",
      "Epoch 10/30\n",
      "97s - loss: 1.3616 - acc: 0.5471 - val_loss: 1.5607 - val_acc: 0.4840\n",
      "Epoch 11/30\n",
      "98s - loss: 1.3448 - acc: 0.5511 - val_loss: 1.4767 - val_acc: 0.5160\n",
      "Epoch 12/30\n",
      "97s - loss: 1.3098 - acc: 0.5716 - val_loss: 1.4656 - val_acc: 0.5320\n",
      "Epoch 13/30\n",
      "98s - loss: 1.2972 - acc: 0.5682 - val_loss: 1.5252 - val_acc: 0.5100\n",
      "Epoch 14/30\n",
      "98s - loss: 1.2786 - acc: 0.5676 - val_loss: 1.3648 - val_acc: 0.5660\n",
      "Epoch 15/30\n",
      "97s - loss: 1.2445 - acc: 0.5862 - val_loss: 1.4794 - val_acc: 0.4960\n",
      "Epoch 16/30\n",
      "97s - loss: 1.2301 - acc: 0.5862 - val_loss: 1.5050 - val_acc: 0.5000\n",
      "Epoch 17/30\n",
      "96s - loss: 1.2217 - acc: 0.5978 - val_loss: 1.3974 - val_acc: 0.5460\n",
      "Epoch 18/30\n",
      "97s - loss: 1.2036 - acc: 0.6036 - val_loss: 1.5693 - val_acc: 0.4760\n",
      "Epoch 19/30\n",
      "97s - loss: 1.1978 - acc: 0.6027 - val_loss: 1.2508 - val_acc: 0.6120\n",
      "Epoch 20/30\n",
      "97s - loss: 1.1794 - acc: 0.6100 - val_loss: 1.2722 - val_acc: 0.5980\n",
      "Epoch 21/30\n",
      "98s - loss: 1.1641 - acc: 0.6155 - val_loss: 1.3201 - val_acc: 0.5700\n",
      "Epoch 22/30\n",
      "97s - loss: 1.1419 - acc: 0.6225 - val_loss: 1.2919 - val_acc: 0.6020\n",
      "Epoch 23/30\n",
      "98s - loss: 1.1488 - acc: 0.6161 - val_loss: 1.2877 - val_acc: 0.5880\n",
      "Epoch 24/30\n",
      "97s - loss: 1.1261 - acc: 0.6240 - val_loss: 1.3093 - val_acc: 0.5740\n",
      "Epoch 25/30\n",
      "97s - loss: 1.1166 - acc: 0.6277 - val_loss: 1.2641 - val_acc: 0.6140\n",
      "Epoch 26/30\n",
      "101s - loss: 1.1133 - acc: 0.6350 - val_loss: 1.1605 - val_acc: 0.6360\n",
      "Epoch 27/30\n",
      "99s - loss: 1.0910 - acc: 0.6460 - val_loss: 1.1927 - val_acc: 0.6380\n",
      "Epoch 28/30\n",
      "99s - loss: 1.0994 - acc: 0.6356 - val_loss: 1.1307 - val_acc: 0.6420\n",
      "Epoch 29/30\n",
      "98s - loss: 1.0728 - acc: 0.6439 - val_loss: 1.1809 - val_acc: 0.6220\n",
      "Epoch 30/30\n",
      "97s - loss: 1.0639 - acc: 0.6512 - val_loss: 1.2590 - val_acc: 0.6000\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def train_all():    \n",
    "    model_paths = {\n",
    "        \"vggbn\": [],\n",
    "        \"inception\": [],\n",
    "        'resnet': [],\n",
    "    }\n",
    "    \n",
    "    for run in range(nb_runs):\n",
    "        print(\"Starting Training Run {0} of {1}...\\n\".format(run+1, nb_runs))\n",
    "        aug_str = \"aug\" if nb_aug else \"no-aug\"\n",
    "        \n",
    "        for arch in archs:\n",
    "            print(\"Training {} model...\\n\".format(arch))\n",
    "            model = models[arch]\n",
    "            model_str = \"{0}x{1}_{2}_{3}lr_run{4}_{5}.h5\".format(model.size[0], model.size[1], aug_str,\n",
    "                                                                 model.lr, run, arch)\n",
    "            model_path = train(model, model_str)\n",
    "            model_paths[arch].append(model_path)\n",
    "        \n",
    "    print(\"Done.\") \n",
    "    return model_paths\n",
    "        \n",
    "model_paths = train_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Prediction Run 1 of 5...\n",
      "\n",
      "\n",
      "--Predicting on Augmentation 1 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 2 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 3 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 4 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 5 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "Starting Prediction Run 2 of 5...\n",
      "\n",
      "\n",
      "--Predicting on Augmentation 1 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 2 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 3 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 4 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 5 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "Starting Prediction Run 3 of 5...\n",
      "\n",
      "\n",
      "--Predicting on Augmentation 1 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 2 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 3 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 4 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 5 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "Starting Prediction Run 4 of 5...\n",
      "\n",
      "\n",
      "--Predicting on Augmentation 1 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 2 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 3 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 4 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 5 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "Starting Prediction Run 5 of 5...\n",
      "\n",
      "\n",
      "--Predicting on Augmentation 1 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 2 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 3 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 4 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "\n",
      "--Predicting on Augmentation 5 of 5...\n",
      "\n",
      "----Predicting on inception model...\n",
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def test(model_paths):    \n",
    "    predictions_full = np.zeros((nb_test_samples, nb_classes))\n",
    "    \n",
    "    for run in range(nb_runs):\n",
    "        print(\"\\nStarting Prediction Run {0} of {1}...\\n\".format(run+1, nb_runs))\n",
    "        predictions_aug = np.zeros((nb_test_samples, nb_classes))\n",
    "        \n",
    "        for aug in range(nb_aug):\n",
    "            print(\"\\n--Predicting on Augmentation {0} of {1}...\\n\".format(aug+1, nb_aug))\n",
    "            predictions_mod = np.zeros((nb_test_samples, nb_classes))\n",
    "            \n",
    "            for arch in archs:\n",
    "                print(\"----Predicting on {} model...\".format(arch))\n",
    "                parent = models[arch]\n",
    "                model = parent.build()\n",
    "                model.load_weights(model_paths[arch][run])\n",
    "                pred, filenames = parent.test(test_path, nb_test_samples, aug=nb_aug)\n",
    "                predictions_mod += pred\n",
    "            \n",
    "            predictions_mod /= len(archs)\n",
    "            predictions_aug += predictions_mod\n",
    "\n",
    "        predictions_aug /= nb_aug\n",
    "        predictions_full += predictions_aug\n",
    "    \n",
    "    predictions_full /= nb_runs\n",
    "    return predictions_full, filenames\n",
    "\n",
    "predictions, filenames = test(model_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Predictions to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Predictions to CSV...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def write_submission(predictions, filenames):\n",
    "    preds = np.clip(predictions, clip, 1-clip)\n",
    "    sub_fn = submission_path + '{0}epoch_{1}aug_{2}clip_{3}runs'.format(nb_epoch, nb_aug, clip, nb_runs)\n",
    "    \n",
    "    for arch in archs:\n",
    "        sub_fn += \"_{}\".format(arch)\n",
    "\n",
    "    with open(sub_fn + '.csv', 'w') as f:\n",
    "        print(\"Writing Predictions to CSV...\")\n",
    "        f.write('image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\\n')\n",
    "        for i, image_name in enumerate(filenames):\n",
    "            pred = ['%.6f' % p for p in preds[i, :]]\n",
    "            f.write('%s,%s\\n' % (os.path.basename(image_name), ','.join(pred)))\n",
    "        print(\"Done.\")\n",
    "\n",
    "write_submission(predictions, filenames)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
