{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nature Conservancy Fish Classification - Bounding Box End-to-End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980M (CNMeM is enabled with initial size: 90.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import GlobalAveragePooling2D, Activation, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from vgg16bn import Vgg16BN as VggConv\n",
    "\n",
    "from utils import * \n",
    "from models import Vgg16BN, Inception, Resnet50\n",
    "from glob import iglob\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_HOME_DIR = ROOT_DIR + '/data'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "data_path = DATA_HOME_DIR + '/cropped/' \n",
    "split_train_path = data_path + '/train/'\n",
    "valid_path = data_path + '/valid/'\n",
    "test_path = DATA_HOME_DIR + '/test/'\n",
    "saved_model_path = ROOT_DIR + '/models/bb_end_to_end/'\n",
    "submission_path = ROOT_DIR + '/submissions/bb_end_to_end/'\n",
    "fish_detector_path = ROOT_DIR + '/models/fish_detector_480x270/0.03-loss_2epoch_480x270_0.3-dropout_0.001-lr_vggbn.h5'\n",
    "bb_regressor_path = 'models/bb_regressor/360x640/loss-1103.20_vgg16_bn.h5'\n",
    "\n",
    "# data\n",
    "batch_size = 16\n",
    "im_size = (224, 224)  # (ht, wt); only 299x299 for inception\n",
    "nb_split_train_samples = 2847\n",
    "nb_valid_samples = 450\n",
    "nb_test_samples = 1000\n",
    "classes = [\"ALB\", \"BET\", \"DOL\", \"LAG\", \"OTHER\", \"SHARK\", \"YFT\"]\n",
    "nb_classes = len(classes)\n",
    "\n",
    "# model\n",
    "nb_runs = 1\n",
    "nb_epoch = 12\n",
    "nb_aug = 1\n",
    "dropout = 0.5\n",
    "lr=0.001           \n",
    "clip = 0.01\n",
    "archs = [\"vggbn\"]\n",
    "\n",
    "models = {\n",
    "    \"vggbn\": Vgg16BN(size=im_size, n_classes=nb_classes, lr=lr,\n",
    "                           batch_size=batch_size, dropout=dropout),\n",
    "    \"inception\": Inception(size=(299, 299), n_classes=nb_classes,\n",
    "                           lr=0.001, batch_size=batch_size),\n",
    "    \"resnet\": Resnet50(size=im_size, n_classes=nb_classes, lr=lr,\n",
    "                    batch_size=batch_size, dropout=dropout)\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(parent_model, model_str):\n",
    "    parent_model.build()    \n",
    "    model_fn = saved_model_path + '{val_loss:.2f}-loss_{epoch}epoch_' + model_str\n",
    "    ckpt = ModelCheckpoint(filepath=model_fn, monitor='val_loss',\n",
    "                           save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    parent_model.fit_val(split_train_path, valid_path, nb_trn_samples=nb_split_train_samples, \n",
    "                         nb_val_samples=nb_valid_samples, nb_epoch=nb_epoch, callbacks=[ckpt], aug=nb_aug)\n",
    "\n",
    "    model_path = max(iglob(saved_model_path + '*.h5'), key=os.path.getctime)\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Run 1 of 1...\n",
      "\n",
      "Training vggbn model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n",
      "/home/robert/anaconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode='max')\n",
      "/home/robert/anaconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2847 images belonging to 7 classes.\n",
      "Found 450 images belonging to 7 classes.\n",
      "Epoch 1/12\n",
      "74s - loss: 1.9374 - acc: 0.6301 - val_loss: 2.6997 - val_acc: 0.6044\n",
      "Epoch 2/12\n",
      "74s - loss: 0.9882 - acc: 0.7921 - val_loss: 2.3606 - val_acc: 0.5889\n",
      "Epoch 3/12\n",
      "74s - loss: 0.7004 - acc: 0.8430 - val_loss: 1.5715 - val_acc: 0.6867\n",
      "Epoch 4/12\n",
      "74s - loss: 0.5866 - acc: 0.8507 - val_loss: 0.7402 - val_acc: 0.8467\n",
      "Epoch 5/12\n",
      "74s - loss: 0.4156 - acc: 0.8901 - val_loss: 0.4452 - val_acc: 0.9022\n",
      "Epoch 6/12\n",
      "73s - loss: 0.3734 - acc: 0.9024 - val_loss: 0.5564 - val_acc: 0.8378\n",
      "Epoch 7/12\n",
      "74s - loss: 0.3237 - acc: 0.9178 - val_loss: 0.3599 - val_acc: 0.9111\n",
      "Epoch 8/12\n",
      "73s - loss: 0.2683 - acc: 0.9308 - val_loss: 0.7596 - val_acc: 0.8244\n",
      "Epoch 9/12\n",
      "73s - loss: 0.2507 - acc: 0.9305 - val_loss: 0.5211 - val_acc: 0.8600\n",
      "Epoch 10/12\n",
      "74s - loss: 0.2346 - acc: 0.9347 - val_loss: 0.3491 - val_acc: 0.9022\n",
      "Epoch 11/12\n",
      "73s - loss: 0.2288 - acc: 0.9375 - val_loss: 0.5186 - val_acc: 0.8911\n",
      "Epoch 12/12\n",
      "73s - loss: 0.1766 - acc: 0.9463 - val_loss: 1.3305 - val_acc: 0.7756\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def train_all():    \n",
    "    model_paths = {\n",
    "        \"vggbn\": [],\n",
    "        \"inception\": [],\n",
    "        'resnet': [],\n",
    "    }\n",
    "    \n",
    "    for run in range(nb_runs):\n",
    "        print(\"Starting Training Run {0} of {1}...\\n\".format(run+1, nb_runs))\n",
    "        aug_str = \"aug\" if nb_aug else \"no-aug\"\n",
    "        \n",
    "        for arch in archs:\n",
    "            print(\"Training {} model...\\n\".format(arch))\n",
    "            model = models[arch]\n",
    "            model_str = \"{0}x{1}_{2}_{3}lr_run{4}_{5}.h5\".format(model.size[0], model.size[1], aug_str,\n",
    "                                                                 model.lr, run, arch)\n",
    "            model_path = train(model, model_str)\n",
    "            model_paths[arch].append(model_path)\n",
    "        \n",
    "    print(\"Done.\") \n",
    "    return model_paths\n",
    "        \n",
    "model_paths = train_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bb_pred(test_image):  \n",
    "    \n",
    "    # load conv model\n",
    "    vgg_conv = VggConv((360, 640)).model\n",
    "    vgg_conv.pop()\n",
    "    vgg_conv.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # build bb regressor & load weights\n",
    "    input_shape = (512, 22, 40)  # shape of output of conv model\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(MaxPooling2D(input_shape=input_shape))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(4))\n",
    "\n",
    "    model.compile(Adam(lr=0.001), loss='mse')\n",
    "    model.load_weights(bb_regressor_path)\n",
    "    \n",
    "    # extract conv features from conv model\n",
    "    conv_test_feat = vgg_conv.predict(test_image)\n",
    "    \n",
    "    # predict bounding box coordinates\n",
    "    bb_preds = model.predict(conv_test_feat)\n",
    "    \n",
    "    return bb_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_preds(model):\n",
    "    fish_detector = Vgg16BN(size=(270, 480), n_classes=2, lr=0.001,\n",
    "                            batch_size=batch_size, dropout=dropout)\n",
    "    fish_detector.build()\n",
    "    fish_detector.model.load_weights(fish_detector_path)\n",
    "\n",
    "    nofish_prob, filenames = fish_detector.test(test_path, nb_test_samples, aug=nb_aug)\n",
    "    nofish_prob = nofish_prob[:, 1]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for ix, fn in enumerate(filenames):\n",
    "        \n",
    "        if ix % 100 == 0:\n",
    "            print(\"Processing {0} of {1}\".format(ix, len(filenames)))\n",
    "        \n",
    "        im_id = fn.split(\"/\")[-1]\n",
    "        im = Image.open(test_path + fn).resize((640, 360))\n",
    "        test_x = np.array(im)\n",
    "    \n",
    "        # Nofish\n",
    "        if nofish_prob[ix] > 0.5:\n",
    "            alloc = (1. - nofish_prob[ix] / 7.)\n",
    "            pred = np.array([alloc, alloc, alloc, alloc, nofish_prob[ix], alloc, alloc, alloc])\n",
    "            predictions.append(pred)\n",
    "            \n",
    "        # Fish\n",
    "        else:\n",
    "            coords = get_bb_pred(test_x.reshape(1, 3, 360, 640))        \n",
    "            ht = coords[0]\n",
    "            wt = coords[1]\n",
    "            x = coords[2]\n",
    "            y = coords[3]\n",
    "            mx_dim = max([wt, ht])\n",
    "\n",
    "            cropped = im.crop((x, y, x + mx_dim, y + mx_dim)).resize((im_size[1], im_size[0]))\n",
    "            test_x = np.array(cropped).reshape(1, 3, im_size[0], im_size[1])\n",
    "            weight = -1. * (nofish_prob[ix] - 1.)\n",
    "            \n",
    "            pred = model.predict(test_x)\n",
    "            pred = weight * pred\n",
    "            pred = np.insert(pred, 4, nofish_prob[ix], axis=1)\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    return np.array(predictions), filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Prediction Run 1 of 1...\n",
      "\n",
      "\n",
      "--Predicting on Augmentation 1 of 1...\n",
      "\n",
      "----Predicting on vggbn model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n",
      "/home/robert/anaconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode='max')\n",
      "/home/robert/anaconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Processing 0 of 1000\n",
      "Processing 100 of 1000\n",
      "Processing 200 of 1000\n",
      "Processing 300 of 1000\n",
      "Processing 400 of 1000\n",
      "Processing 500 of 1000\n",
      "Processing 600 of 1000\n",
      "Processing 700 of 1000\n",
      "Processing 800 of 1000\n",
      "Processing 900 of 1000\n"
     ]
    }
   ],
   "source": [
    "def test(model_paths):   \n",
    "                 \n",
    "    print(\"----Predicting on {} model...\".format(arch))\n",
    "    parent = models[\"vggbn\"]\n",
    "    model = parent.build()\n",
    "    model.load_weights(model_paths[arch][run])\n",
    "    pred, filenames = generate_preds(model)\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    # weird, hacky reshaping\n",
    "    for p in pred:\n",
    "        if p.shape != (1, 8):\n",
    "            p = p.reshape(1, 8)\n",
    "        predictions.append(p)\n",
    "        \n",
    "    predictions = np.concatenate([p for p in predictions])\n",
    "    return predictions, filenames\n",
    "\n",
    "predictions, filenames = test(model_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Predictions to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Predictions to CSV...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def write_submission(predictions, filenames):\n",
    "    preds = np.clip(predictions, clip, 1-clip)\n",
    "    sub_fn = submission_path + '{0}epoch_{1}aug_{2}clip_{3}runs'.format(nb_epoch, nb_aug, clip, nb_runs)\n",
    "    \n",
    "    for arch in archs:\n",
    "        sub_fn += \"_{}\".format(arch)\n",
    "\n",
    "    with open(sub_fn + '.csv', 'w') as f:\n",
    "        print(\"Writing Predictions to CSV...\")\n",
    "        f.write('image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\\n')\n",
    "        for i, image_name in enumerate(filenames):\n",
    "            pred = ['%.6f' % p for p in preds[i, :]]\n",
    "            f.write('%s,%s\\n' % (os.path.basename(image_name), ','.join(pred)))\n",
    "        print(\"Done.\")\n",
    "\n",
    "write_submission(predicitions, filenames)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
